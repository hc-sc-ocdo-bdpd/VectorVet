{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö VectorVet Demo Notebook  \n",
    "  \n",
    "This notebook demonstrates embedding generation, loading embeddings, computing metrics, and summarizing results using the custom `VectorVet` toolkit. \n",
    "\n",
    "---    \n",
    "## üîß Setup and Imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries  \n",
    "import sys  \n",
    "from pathlib import Path  \n",
    "  \n",
    "# Data Science Libraries  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from tqdm.auto import tqdm  \n",
    "  \n",
    "# Text Processing  \n",
    "from sklearn.datasets import fetch_20newsgroups  \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "  \n",
    "# Embedding Generation  \n",
    "from llama_cpp import Llama  \n",
    "  \n",
    "# Custom VectorVet Modules  \n",
    "PROJECT_ROOT = Path.cwd().parent  \n",
    "sys.path.append(str(PROJECT_ROOT))  \n",
    "  \n",
    "from vectorvet.core.loader import load_multiple_embeddings\n",
    "from vectorvet.core.metrics import run_all_metrics\n",
    "from vectorvet.core.summarizer import summarize_to_dataframe\n",
    "from vectorvet.core.utils import timer\n",
    "  \n",
    "# Display Configuration  \n",
    "pd.set_option(\"display.max_columns\", None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## üóÉÔ∏è Load and Chunk the Dataset  \n",
    "  \n",
    "We'll use the well-known 20 Newsgroups dataset for this demo. We'll chunk the texts into manageable pieces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the 20 Newsgroups dataset  \n",
    "news = fetch_20newsgroups(subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))  \n",
    "texts = [t for t in news.data if t.strip()]  \n",
    "  \n",
    "# Initialize a text splitter  \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=50)  \n",
    "  \n",
    "# Chunk the dataset  \n",
    "chunked_data = []  \n",
    "for idx, text in enumerate(tqdm(texts, desc=\"Chunking texts\")):  \n",
    "    chunks = splitter.split_text(text)  \n",
    "    for chunk_idx, chunk in enumerate(chunks):  \n",
    "        chunked_data.append({  \n",
    "            \"original_index\": idx,  \n",
    "            \"chunk_index\": chunk_idx,  \n",
    "            \"chunk\": chunk  \n",
    "        })  \n",
    "  \n",
    "# Create DataFrame of chunks  \n",
    "chunked_df = pd.DataFrame(chunked_data)  \n",
    "chunk_texts = chunked_df[\"chunk\"].tolist()  \n",
    "  \n",
    "print(f\"‚úÖ Total chunks created: {len(chunk_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_texts = chunk_texts[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## üìå Generate Embeddings for Each Model  \n",
    "  \n",
    "We'll generate embeddings using various LLM models and save them for further analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths  \n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"  \n",
    "EMB_DIR = PROJECT_ROOT / \"embeddings\"  \n",
    "EMB_DIR.mkdir(exist_ok=True, parents=True)  \n",
    "  \n",
    "# List of models to embed with  \n",
    "MODELS = [  \n",
    "    \"Phi-3-mini-4k-instruct-q4.gguf\",  \n",
    "    \"Llama-3.2-1B-Instruct.Q6_K.gguf\",  \n",
    "    \"Llama-3.1-8b-instruct-q6_k.gguf\",  \n",
    "    \"phi-2.Q6_K.gguf\",  \n",
    "]  \n",
    "  \n",
    "# Generate embeddings  \n",
    "for fname in MODELS:  \n",
    "    model_path = MODEL_DIR / fname  \n",
    "    model_name = model_path.stem  \n",
    "    out_file = EMB_DIR / f\"{model_name}_20news_chunks.npy\"  \n",
    "  \n",
    "    if out_file.exists():  \n",
    "        print(f\"‚úî {out_file.name} already exists ‚Äì skipping\")  \n",
    "        continue  \n",
    "  \n",
    "    print(f\"‚Üí Embedding with {model_name} ‚Ä¶\")  \n",
    "    llm = Llama(  \n",
    "        model_path=str(model_path),  \n",
    "        n_gpu_layers=-1,  \n",
    "        embedding=True,  \n",
    "    )  \n",
    "  \n",
    "    embs = np.zeros((len(chunk_texts), llm.n_embd()), dtype=np.float32)  \n",
    "  \n",
    "    with timer(f\"Embedding generation for {model_name}\"):  \n",
    "        for i, txt in enumerate(tqdm(chunk_texts, desc=f\"Embedding ({model_name})\")):  \n",
    "            emb = llm.embed(txt)  \n",
    "            emb = np.array(emb)  \n",
    "  \n",
    "            if emb.ndim > 1:  \n",
    "                emb = emb.mean(axis=0)  # Average if needed  \n",
    "  \n",
    "            emb = emb.flatten()  \n",
    "  \n",
    "            if emb.shape[0] != llm.n_embd():  \n",
    "                print(f\"Warning: Skipping text {i} due to embedding size mismatch: {emb.shape}\")  \n",
    "                continue  \n",
    "  \n",
    "            embs[i] = emb  \n",
    "  \n",
    "    np.save(out_file, embs)  \n",
    "    print(f\"‚úî Saved embeddings to {out_file}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## üìä Compute Metrics and Summarize Results  \n",
    "  \n",
    "Now let's load the embeddings we've saved, compute intrinsic embedding metrics, and summarize results into a tidy dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find saved embeddings  \n",
    "\n",
    "files = {  \n",
    "    p.stem.split(\"_20news_chunks\")[0]: str(p)  \n",
    "    for p in EMB_DIR.glob(\"*_20news_chunks.npy\")  \n",
    "}  \n",
    "  \n",
    "print(\"üóÇÔ∏è Embedding sets detected:\", list(files.keys()))  \n",
    "  \n",
    "# Load embeddings  \n",
    "embs = load_multiple_embeddings(files)  \n",
    "  \n",
    "# Compute all metrics  \n",
    "results = {}  \n",
    "for name, mat in embs.items():  \n",
    "    print(f\"\\nüìê Running metrics for: {name}\")  \n",
    "    with timer(f\"Metrics computation for {name}\"):  \n",
    "        results[name] = run_all_metrics(mat)  \n",
    "  \n",
    "# Summarize results into DataFrame  \n",
    "summary_df = summarize_to_dataframe(results)  \n",
    "  \n",
    "# Display summarized metrics nicely  \n",
    "summary_df.style.format(precision=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## ‚úÖ Final Results  \n",
    "  \n",
    "The resulting table summarizes the embedding quality across all models, making it easy to compare and interpret metrics like isotropy, hubness, clustering quality, and pairwise cosine similarity.  \n",
    "  \n",
    "üéâ **You're all set!**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
