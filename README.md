# üß¨ VectorVet: Quick Embedding Diagnostics for Semantic Search & RAG Pipelines  
  
VectorVet provides simple, practical metrics and tooling for evaluating the quality of embedding vectors generated by local language models. It's especially useful for quickly assessing whether embeddings from smaller or quantized models (such as those that fit on a laptop GPU) are suitable for semantic search and Retrieval-Augmented Generation (RAG) applications.  
  
The library calculates intrinsic embedding metrics‚Äîsuch as isotropy, hubness, clustering quality, and cosine similarity statistics‚Äîto help you quickly determine whether your embeddings are well-structured for semantic search.  
  
---  
  
## üö© Why VectorVet?  
  
When building internal RAG chatbots or semantic search systems, embedding quality is crucial. Smaller or quantized models are attractive for privacy and portability, but their embeddings can often suffer from poor distribution or semantic structure.  
  
VectorVet helps you rapidly measure embedding quality, giving you early insights into whether your embeddings are likely to support effective semantic search.  
  
**Quickly answer questions like:**  
  
- **Are my embeddings well-distributed (isotropic)?**  
- **Do my embeddings suffer from "hubness" problems?**  
- **Do the embeddings naturally cluster by topic?**  
- **Will semantic search likely yield meaningful results with this embedding model?**  
  
---  
  
## üîç Metrics Explained  
  
VectorVet computes the following intrinsic metrics to assess your embeddings:  
  
### Isotropy  
Measures how evenly embeddings utilize vector space dimensions. An isotropic embedding set uses all directions in vector space, which is desirable for effective semantic search.  
  
- **IsoScore** ‚àà [0,‚ÄØ1]; higher is better.  
  
### Hubness  
Identifies whether certain embeddings ("hubs") appear disproportionately often as nearest neighbors, reducing search quality.  
  
- **Skewness**: Lower is better.  
- **Robin Hood Index**: Lower is better.  
- **Antihub Rate**: Indicates embeddings rarely appearing as nearest neighbors; lower is generally preferable.  
  
### Clustering Quality  
Evaluates how well embeddings naturally form distinct clusters, indicative of good semantic separation.  
  
- **Silhouette Score**: Higher is better.  
- **Davies-Bouldin Index**: Lower is better.  
  
### Pairwise Cosine Similarity  
Provides general statistics (mean and standard deviation) of cosine similarity between embeddings, indicating overall vector similarity distribution.  
  
---  
  
## üìñ Example Usage  
  
For a complete example workflow‚Äîincluding dataset preparation, embedding generation, metric calculations, and results summarization‚Äîplease see the provided Jupyter Notebook demo:  
  
- [`notebooks/vectorvet_demo.ipynb`](notebooks/vectorvet_demo.ipynb)  
  
This notebook demonstrates end-to-end usage of VectorVet, from loading embeddings to interpreting the resulting metrics.  
  
---  

## üí° Interpretation Guidelines  
  
As a general rule of thumb, embeddings with good isotropy, minimal hubness, and clear clustering structure are likely to yield effective semantic search results. Embeddings that fail these checks should be reconsidered or post-processed (e.g., mean centering, whitening, hubness reduction).  
  
Use the following guidelines to interpret your intrinsic metric scores:  
  
| Metric                      | ‚úÖ Good                    | ‚ö†Ô∏è Concerning              | üö´ Poor                      |  
|-----------------------------|----------------------------|----------------------------|------------------------------|  
| **IsoScore**                | ‚â• 0.3 (ideally ‚â• 0.5)      | 0.1‚Äì0.3                    | < 0.1                        |  
| **Hubness: Skewness**       | ‚â§ 1.5                      | 1.5‚Äì3.0                    | > 3.0                        |  
| **Hubness: Robin Hood**     | ‚â§ 0.3                      | 0.3‚Äì0.5                    | > 0.5                        |  
| **Clustering: Silhouette**  | ‚â• 0.2 (ideally ‚â• 0.3)      | 0.05‚Äì0.2                   | < 0.05                       |  
| **Clustering: Davies-Bouldin**| ‚â§ 2.0 (ideally ‚â§ 1.5)    | 2.0‚Äì3.0                    | > 3.0                        |  
| **Cosine Similarity (mean)**| Moderate (0.2‚Äì0.6)         | < 0.2 or 0.6‚Äì0.8           | > 0.8 (embeddings collapsing)|  
| **Cosine Similarity (std)** | ‚â• 0.15                     | 0.05‚Äì0.15                  | < 0.05 (little variation)    |  
  
- **IsoScore**: Higher isotropy (‚â•‚ÄØ0.3) is desirable. Values below 0.1 indicate embeddings overly concentrated in a few directions.  
- **Hubness Metrics**: Lower skewness and Robin Hood index indicate fewer problematic "hub" embeddings. High values mean certain embeddings dominate as nearest neighbors, hurting semantic search.  
- **Clustering Scores**: Embeddings should naturally form clear clusters. Higher silhouette scores and lower Davies-Bouldin indices indicate better separation.  
- **Cosine Similarity**: Embeddings should neither be too similar (mean cosine similarity >‚ÄØ0.8 indicates collapsing) nor completely unrelated (mean cosine similarity <‚ÄØ0.2). A higher standard deviation (‚â•‚ÄØ0.15) generally indicates healthier variation across embeddings.  
  
---  
  
## ‚ö†Ô∏è Caveats  
  
Intrinsic metrics provide a quick and powerful diagnostic tool but are not a guarantee of actual semantic search performance. Always perform at least a minimal end-to-end semantic retrieval test on your specific data and embedding model before committing significant resources.  
  
Additionally, intrinsic metric thresholds are approximate guidelines. Optimal thresholds can vary significantly depending on your dataset domain, use-case, and embedding dimensionality. If possible, benchmark your results against known high-quality embedding models or datasets to calibrate your expectations more accurately.  
  
Intrinsic metrics also depend heavily on the dataset used. A model performing well intrinsically on one dataset might not perform equally well on another. Always run these metrics against embeddings from your actual data or a representative subset of your data.  
  
If embeddings consistently score poorly on several metrics, consider trying other embedding models or applying post-processing techniques (mean centering, PCA whitening, or hubness reduction) to improve quality.  